{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning with PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to train a model that recognizes [MNIST digits](https://en.wikipedia.org/wiki/MNIST_database) using Federated Learning that creates a joint global model from multiple local models trained on user data. Please consult the [original paper](https://arxiv.org/abs/1602.05629) and don't hesitate to ask questions.\n",
    "\n",
    "We provide some skeleton code and a reference implementation of the centralized training from [PyTorch examples](https://github.com/pytorch/examples/blob/master/mnist/main.py).\n",
    "\n",
    "**Your task:** fill up the skeleton code and write a training procedure for FL. \n",
    "\n",
    "Submit this notebook to Canvas and we will run it and examine the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and parameters\n",
    "\n",
    "No need to modify this part but you need to be familiar with the primitives: data loaders, models, central training and testing functions. It's mainly taken from [PyTorch examples](https://github.com/pytorch/examples/blob/master/mnist/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim import SGD\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cpu or cuda\n",
    "use_cuda=False\n",
    "\n",
    "# Learning rate for the update of global model\n",
    "global_lr = 0.1\n",
    "local_lr = 0.1\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# FL parameters\n",
    "no_users = 100\n",
    "no_rounds = 5\n",
    "round_size = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': batch_size}\n",
    "if use_cuda:\n",
    "    device='cuda'\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   }\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "else:\n",
    "    device='cpu'\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training dataset into smaller ones for each participant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(training_dataset, no_users):\n",
    "    data_per_user = len(training_dataset) // no_users\n",
    "    data_loaders = list()\n",
    "    for i in range(no_users):\n",
    "        indices = random.sample(list(range(len(training_dataset))), data_per_user)\n",
    "        sampler = torch.utils.data.SubsetRandomSampler(indices)\n",
    "        data_loader = torch.utils.data.DataLoader(training_dataset, sampler=sampler, **train_kwargs)\n",
    "        data_loaders.append(data_loader)\n",
    "        \n",
    "    return data_loaders\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "\n",
    "data_loaders = split_data(train_dataset, no_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of the global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    \"\"\"Perform testing of the global aggregated model.\n",
    "    \n",
    "    Args:\n",
    "        model: torch.nn global model.\n",
    "        train_loader: loader for global testing data.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "        \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a central training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int, model: Net, train_loader: DataLoader, optimizer: DataLoader):\n",
    "    \"\"\" Centralized training.\n",
    "    \n",
    "    Args:\n",
    "        epoch: training epoch.\n",
    "        model: torch.nn model.\n",
    "        train_loader: loader for global training data.\n",
    "        optimizer: optimizer for global model.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some ideas to help you navigate PyTorch training for FL:\n",
    "\n",
    "1. No need to do parallel training of the models in one round. You can sequentially train each model in a round and accumulate the model weights of trained models.\n",
    "\n",
    "2. Once the result in a round is summed up you can average it and apply to the global model.\n",
    "\n",
    "3. Be careful on copying tensors: participant's model before training needs to be a copy of the global model. It's useful to always keep `global_model` and create a `local_model` for every participant (use `deepcopy()`). The optimizer needs to be recreated for every `local_model`.\n",
    "\n",
    "\n",
    "\n",
    "Primitives:\n",
    "\n",
    "`model.named_parameters()` -- returns a dictionary of layer names and parameter tensors for the model.\n",
    "\n",
    "`param.detach().clone()` -- clone the parameter tensor (useful when accumulating results from multiple models). \n",
    "\n",
    "`param.data.add_(data)` -- modify the value of the weight tensor by adding `data` (useful when updating `global_model`).\n",
    "\n",
    "`copy.deepcopy(global_model)` -- copy the whole model (useful to create a `local_model`).\n",
    "\n",
    "`optimizer = optim.SGD(local_model.parameters(), lr=local_lr)` -- create optimizer for local model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task begins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fill FL primitives\n",
    "\n",
    "Create a `local_train` function to train the model locally, use `accumulate` to sum local models into one object, and `average` to average these models and update the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def local_train(model_id: int, global_model: Net, train_loader: DataLoader, local_lr: float) -> Net:\n",
    "    \"\"\"Perform training of the local model on local data.\n",
    "    \n",
    "    Args:\n",
    "        model_id: identificator of the local model.\n",
    "        global_model: global model (cannot be modified!).\n",
    "        train_loader: loader for local training data.\n",
    "        local_lr: learning rate for the local optimizer.\n",
    "    \n",
    "    Returns:\n",
    "        Trained local model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (Hint: modify centralized training function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(local_model: Net, weight_aggregator: Dict[str, Parameter]) -> Dict[str, Parameter]:\n",
    "    \"\"\"Accumulate local model into a weight_aggregator.\n",
    "    \n",
    "    Args:\n",
    "        local_model: User trained local model\n",
    "        weight_aggregator: sum of all local models from the single round.\n",
    "    \n",
    "    Returns: \n",
    "        updated weight aggregator\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(no_models: int, global_model: Net, weight_aggregator: Dict[str, Parameter], global_lr: float) -> Net:\n",
    "    \"\"\"Average accumulated models and apply them to the global model.\n",
    "    \n",
    "    Args:\n",
    "        global_model: Server's FL model\n",
    "        no_models: number of models in a single FL round\n",
    "        weight_aggregator: sum of all local models from the single round\n",
    "        global_lr: learning rate to update the global model.\n",
    "    \n",
    "    Returns:\n",
    "        Updated global model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run FL training and testing\n",
    "\n",
    "\n",
    "Using above primitives implement Federated Learning routine by training \n",
    "a global model for `no_rounds` and sampling `round_size` users from `no_user` for each round.\n",
    "\n",
    "Don't forget to test the `global_model` on convergence. Successfully trained global model will score above 75% on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL Round: 0\n",
      "Training model: 57\n",
      "Train Model: 57 [1/19 (5%)]\tLoss: 2.339928\n",
      "Train Model: 57 [6/19 (32%)]\tLoss: 2.188959\n",
      "Train Model: 57 [11/19 (58%)]\tLoss: 1.888498\n",
      "Train Model: 57 [16/19 (84%)]\tLoss: 1.718906\n",
      "Train Model: 57 [19/19 (100%)]\tLoss: 1.918045\n",
      "Training model: 17\n",
      "Train Model: 17 [1/19 (5%)]\tLoss: 2.309593\n",
      "Train Model: 17 [6/19 (32%)]\tLoss: 2.099994\n",
      "Train Model: 17 [11/19 (58%)]\tLoss: 2.212420\n",
      "Train Model: 17 [16/19 (84%)]\tLoss: 1.324640\n",
      "Train Model: 17 [19/19 (100%)]\tLoss: 1.694795\n",
      "Training model: 75\n",
      "Train Model: 75 [1/19 (5%)]\tLoss: 2.267133\n",
      "Train Model: 75 [6/19 (32%)]\tLoss: 2.164688\n",
      "Train Model: 75 [11/19 (58%)]\tLoss: 1.570859\n",
      "Train Model: 75 [16/19 (84%)]\tLoss: 2.067035\n",
      "Train Model: 75 [19/19 (100%)]\tLoss: 1.766402\n",
      "Training model: 55\n",
      "Train Model: 55 [1/19 (5%)]\tLoss: 2.334300\n",
      "Train Model: 55 [6/19 (32%)]\tLoss: 2.090713\n",
      "Train Model: 55 [11/19 (58%)]\tLoss: 1.930814\n",
      "Train Model: 55 [16/19 (84%)]\tLoss: 2.170949\n",
      "Train Model: 55 [19/19 (100%)]\tLoss: 1.365606\n",
      "Training model: 7\n",
      "Train Model: 7 [1/19 (5%)]\tLoss: 2.308432\n",
      "Train Model: 7 [6/19 (32%)]\tLoss: 2.113135\n",
      "Train Model: 7 [11/19 (58%)]\tLoss: 1.818022\n",
      "Train Model: 7 [16/19 (84%)]\tLoss: 1.584904\n",
      "Train Model: 7 [19/19 (100%)]\tLoss: 1.681297\n",
      "Training model: 61\n",
      "Train Model: 61 [1/19 (5%)]\tLoss: 2.283662\n",
      "Train Model: 61 [6/19 (32%)]\tLoss: 2.174047\n",
      "Train Model: 61 [11/19 (58%)]\tLoss: 1.933015\n",
      "Train Model: 61 [16/19 (84%)]\tLoss: 1.603393\n",
      "Train Model: 61 [19/19 (100%)]\tLoss: 1.681989\n",
      "Training model: 16\n",
      "Train Model: 16 [1/19 (5%)]\tLoss: 2.328933\n",
      "Train Model: 16 [6/19 (32%)]\tLoss: 2.166938\n",
      "Train Model: 16 [11/19 (58%)]\tLoss: 1.642523\n",
      "Train Model: 16 [16/19 (84%)]\tLoss: 1.970709\n",
      "Train Model: 16 [19/19 (100%)]\tLoss: 1.431241\n",
      "Training model: 85\n",
      "Train Model: 85 [1/19 (5%)]\tLoss: 2.310678\n",
      "Train Model: 85 [6/19 (32%)]\tLoss: 1.965083\n",
      "Train Model: 85 [11/19 (58%)]\tLoss: 2.016690\n",
      "Train Model: 85 [16/19 (84%)]\tLoss: 1.636144\n",
      "Train Model: 85 [19/19 (100%)]\tLoss: 1.857235\n",
      "Training model: 53\n",
      "Train Model: 53 [1/19 (5%)]\tLoss: 2.341029\n",
      "Train Model: 53 [6/19 (32%)]\tLoss: 2.071247\n",
      "Train Model: 53 [11/19 (58%)]\tLoss: 1.874356\n",
      "Train Model: 53 [16/19 (84%)]\tLoss: 1.571278\n",
      "Train Model: 53 [19/19 (100%)]\tLoss: 1.841043\n",
      "Training model: 22\n",
      "Train Model: 22 [1/19 (5%)]\tLoss: 2.289503\n",
      "Train Model: 22 [6/19 (32%)]\tLoss: 2.096182\n",
      "Train Model: 22 [11/19 (58%)]\tLoss: 2.127289\n",
      "Train Model: 22 [16/19 (84%)]\tLoss: 1.355264\n",
      "Train Model: 22 [19/19 (100%)]\tLoss: 2.005380\n",
      "\n",
      "Test set: Average loss: 2.2092, Accuracy: 3639/10000 (36%)\n",
      "\n",
      "FL Round: 1\n",
      "Training model: 27\n",
      "Train Model: 27 [1/19 (5%)]\tLoss: 2.236460\n",
      "Train Model: 27 [6/19 (32%)]\tLoss: 2.242172\n",
      "Train Model: 27 [11/19 (58%)]\tLoss: 1.742164\n",
      "Train Model: 27 [16/19 (84%)]\tLoss: 1.635964\n",
      "Train Model: 27 [19/19 (100%)]\tLoss: 1.435146\n",
      "Training model: 9\n",
      "Train Model: 9 [1/19 (5%)]\tLoss: 2.253153\n",
      "Train Model: 9 [6/19 (32%)]\tLoss: 1.965003\n",
      "Train Model: 9 [11/19 (58%)]\tLoss: 1.818606\n",
      "Train Model: 9 [16/19 (84%)]\tLoss: 1.742105\n",
      "Train Model: 9 [19/19 (100%)]\tLoss: 1.563972\n",
      "Training model: 69\n",
      "Train Model: 69 [1/19 (5%)]\tLoss: 2.214455\n",
      "Train Model: 69 [6/19 (32%)]\tLoss: 1.881904\n",
      "Train Model: 69 [11/19 (58%)]\tLoss: 1.921867\n",
      "Train Model: 69 [16/19 (84%)]\tLoss: 1.624922\n",
      "Train Model: 69 [19/19 (100%)]\tLoss: 1.564210\n",
      "Training model: 47\n",
      "Train Model: 47 [1/19 (5%)]\tLoss: 2.225753\n",
      "Train Model: 47 [6/19 (32%)]\tLoss: 2.115421\n",
      "Train Model: 47 [11/19 (58%)]\tLoss: 1.851432\n",
      "Train Model: 47 [16/19 (84%)]\tLoss: 1.678674\n",
      "Train Model: 47 [19/19 (100%)]\tLoss: 1.481370\n",
      "Training model: 11\n",
      "Train Model: 11 [1/19 (5%)]\tLoss: 2.215400\n",
      "Train Model: 11 [6/19 (32%)]\tLoss: 1.919102\n",
      "Train Model: 11 [11/19 (58%)]\tLoss: 1.698990\n",
      "Train Model: 11 [16/19 (84%)]\tLoss: 1.396724\n",
      "Train Model: 11 [19/19 (100%)]\tLoss: 1.255409\n",
      "Training model: 57\n",
      "Train Model: 57 [1/19 (5%)]\tLoss: 2.258757\n",
      "Train Model: 57 [6/19 (32%)]\tLoss: 2.125200\n",
      "Train Model: 57 [11/19 (58%)]\tLoss: 1.931449\n",
      "Train Model: 57 [16/19 (84%)]\tLoss: 1.336425\n",
      "Train Model: 57 [19/19 (100%)]\tLoss: 1.374223\n",
      "Training model: 31\n",
      "Train Model: 31 [1/19 (5%)]\tLoss: 2.226513\n",
      "Train Model: 31 [6/19 (32%)]\tLoss: 2.346169\n",
      "Train Model: 31 [11/19 (58%)]\tLoss: 1.624864\n",
      "Train Model: 31 [16/19 (84%)]\tLoss: 1.379930\n",
      "Train Model: 31 [19/19 (100%)]\tLoss: 1.406510\n",
      "Training model: 42\n",
      "Train Model: 42 [1/19 (5%)]\tLoss: 2.251094\n",
      "Train Model: 42 [6/19 (32%)]\tLoss: 1.976857\n",
      "Train Model: 42 [11/19 (58%)]\tLoss: 1.619590\n",
      "Train Model: 42 [16/19 (84%)]\tLoss: 1.304672\n",
      "Train Model: 42 [19/19 (100%)]\tLoss: 0.999167\n",
      "Training model: 97\n",
      "Train Model: 97 [1/19 (5%)]\tLoss: 2.206613\n",
      "Train Model: 97 [6/19 (32%)]\tLoss: 1.806188\n",
      "Train Model: 97 [11/19 (58%)]\tLoss: 1.814684\n",
      "Train Model: 97 [16/19 (84%)]\tLoss: 1.474878\n",
      "Train Model: 97 [19/19 (100%)]\tLoss: 1.369952\n",
      "Training model: 51\n",
      "Train Model: 51 [1/19 (5%)]\tLoss: 2.237383\n",
      "Train Model: 51 [6/19 (32%)]\tLoss: 1.938587\n",
      "Train Model: 51 [11/19 (58%)]\tLoss: 2.250789\n",
      "Train Model: 51 [16/19 (84%)]\tLoss: 1.818930\n",
      "Train Model: 51 [19/19 (100%)]\tLoss: 1.305310\n",
      "\n",
      "Test set: Average loss: 2.0370, Accuracy: 6480/10000 (65%)\n",
      "\n",
      "FL Round: 2\n",
      "Training model: 92\n",
      "Train Model: 92 [1/19 (5%)]\tLoss: 2.086502\n",
      "Train Model: 92 [6/19 (32%)]\tLoss: 2.188476\n",
      "Train Model: 92 [11/19 (58%)]\tLoss: 1.672092\n",
      "Train Model: 92 [16/19 (84%)]\tLoss: 1.581564\n",
      "Train Model: 92 [19/19 (100%)]\tLoss: 1.777043\n",
      "Training model: 37\n",
      "Train Model: 37 [1/19 (5%)]\tLoss: 2.135600\n",
      "Train Model: 37 [6/19 (32%)]\tLoss: 1.829568\n",
      "Train Model: 37 [11/19 (58%)]\tLoss: 1.394458\n",
      "Train Model: 37 [16/19 (84%)]\tLoss: 1.272926\n",
      "Train Model: 37 [19/19 (100%)]\tLoss: 1.114178\n",
      "Training model: 44\n",
      "Train Model: 44 [1/19 (5%)]\tLoss: 2.078100\n",
      "Train Model: 44 [6/19 (32%)]\tLoss: 1.760995\n",
      "Train Model: 44 [11/19 (58%)]\tLoss: 1.860225\n",
      "Train Model: 44 [16/19 (84%)]\tLoss: 1.661405\n",
      "Train Model: 44 [19/19 (100%)]\tLoss: 1.574419\n",
      "Training model: 54\n",
      "Train Model: 54 [1/19 (5%)]\tLoss: 2.056109\n",
      "Train Model: 54 [6/19 (32%)]\tLoss: 2.210442\n",
      "Train Model: 54 [11/19 (58%)]\tLoss: 1.620497\n",
      "Train Model: 54 [16/19 (84%)]\tLoss: 1.663331\n",
      "Train Model: 54 [19/19 (100%)]\tLoss: 1.570971\n",
      "Training model: 21\n",
      "Train Model: 21 [1/19 (5%)]\tLoss: 2.051931\n",
      "Train Model: 21 [6/19 (32%)]\tLoss: 1.743076\n",
      "Train Model: 21 [11/19 (58%)]\tLoss: 1.223308\n",
      "Train Model: 21 [16/19 (84%)]\tLoss: 1.058549\n",
      "Train Model: 21 [19/19 (100%)]\tLoss: 1.037594\n",
      "Training model: 39\n",
      "Train Model: 39 [1/19 (5%)]\tLoss: 2.045048\n",
      "Train Model: 39 [6/19 (32%)]\tLoss: 2.173250\n",
      "Train Model: 39 [11/19 (58%)]\tLoss: 1.754933\n",
      "Train Model: 39 [16/19 (84%)]\tLoss: 1.252918\n",
      "Train Model: 39 [19/19 (100%)]\tLoss: 0.899512\n",
      "Training model: 61\n",
      "Train Model: 61 [1/19 (5%)]\tLoss: 2.094481\n",
      "Train Model: 61 [6/19 (32%)]\tLoss: 2.231979\n",
      "Train Model: 61 [11/19 (58%)]\tLoss: 1.868249\n",
      "Train Model: 61 [16/19 (84%)]\tLoss: 1.257654\n",
      "Train Model: 61 [19/19 (100%)]\tLoss: 1.155990\n",
      "Training model: 72\n",
      "Train Model: 72 [1/19 (5%)]\tLoss: 2.085314\n",
      "Train Model: 72 [6/19 (32%)]\tLoss: 2.198323\n",
      "Train Model: 72 [11/19 (58%)]\tLoss: 1.979677\n",
      "Train Model: 72 [16/19 (84%)]\tLoss: 1.274157\n",
      "Train Model: 72 [19/19 (100%)]\tLoss: 1.336757\n",
      "Training model: 53\n",
      "Train Model: 53 [1/19 (5%)]\tLoss: 2.060724\n",
      "Train Model: 53 [6/19 (32%)]\tLoss: 2.133938\n",
      "Train Model: 53 [11/19 (58%)]\tLoss: 2.012094\n",
      "Train Model: 53 [16/19 (84%)]\tLoss: 1.425076\n",
      "Train Model: 53 [19/19 (100%)]\tLoss: 1.118065\n",
      "Training model: 41\n",
      "Train Model: 41 [1/19 (5%)]\tLoss: 2.136380\n",
      "Train Model: 41 [6/19 (32%)]\tLoss: 1.818642\n",
      "Train Model: 41 [11/19 (58%)]\tLoss: 1.577935\n",
      "Train Model: 41 [16/19 (84%)]\tLoss: 1.255167\n",
      "Train Model: 41 [19/19 (100%)]\tLoss: 1.332657\n",
      "\n",
      "Test set: Average loss: 1.8290, Accuracy: 7178/10000 (72%)\n",
      "\n",
      "FL Round: 3\n",
      "Training model: 95\n",
      "Train Model: 95 [1/19 (5%)]\tLoss: 1.921530\n",
      "Train Model: 95 [6/19 (32%)]\tLoss: 1.674785\n",
      "Train Model: 95 [11/19 (58%)]\tLoss: 1.609320\n",
      "Train Model: 95 [16/19 (84%)]\tLoss: 1.168297\n",
      "Train Model: 95 [19/19 (100%)]\tLoss: 1.257462\n",
      "Training model: 81\n",
      "Train Model: 81 [1/19 (5%)]\tLoss: 1.945173\n",
      "Train Model: 81 [6/19 (32%)]\tLoss: 1.926856\n",
      "Train Model: 81 [11/19 (58%)]\tLoss: 1.350777\n",
      "Train Model: 81 [16/19 (84%)]\tLoss: 0.917858\n",
      "Train Model: 81 [19/19 (100%)]\tLoss: 1.271414\n",
      "Training model: 18\n",
      "Train Model: 18 [1/19 (5%)]\tLoss: 1.910080\n",
      "Train Model: 18 [6/19 (32%)]\tLoss: 2.098191\n",
      "Train Model: 18 [11/19 (58%)]\tLoss: 1.610618\n",
      "Train Model: 18 [16/19 (84%)]\tLoss: 1.147830\n",
      "Train Model: 18 [19/19 (100%)]\tLoss: 0.911945\n",
      "Training model: 53\n",
      "Train Model: 53 [1/19 (5%)]\tLoss: 1.828615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model: 53 [6/19 (32%)]\tLoss: 2.050525\n",
      "Train Model: 53 [11/19 (58%)]\tLoss: 1.494460\n",
      "Train Model: 53 [16/19 (84%)]\tLoss: 1.223925\n",
      "Train Model: 53 [19/19 (100%)]\tLoss: 1.178287\n",
      "Training model: 57\n",
      "Train Model: 57 [1/19 (5%)]\tLoss: 1.905026\n",
      "Train Model: 57 [6/19 (32%)]\tLoss: 2.010834\n",
      "Train Model: 57 [11/19 (58%)]\tLoss: 2.288035\n",
      "Train Model: 57 [16/19 (84%)]\tLoss: 2.183748\n",
      "Train Model: 57 [19/19 (100%)]\tLoss: 1.537451\n",
      "Training model: 50\n",
      "Train Model: 50 [1/19 (5%)]\tLoss: 1.856608\n",
      "Train Model: 50 [6/19 (32%)]\tLoss: 2.230909\n",
      "Train Model: 50 [11/19 (58%)]\tLoss: 1.926368\n",
      "Train Model: 50 [16/19 (84%)]\tLoss: 1.615474\n",
      "Train Model: 50 [19/19 (100%)]\tLoss: 1.165149\n",
      "Training model: 87\n",
      "Train Model: 87 [1/19 (5%)]\tLoss: 1.886182\n",
      "Train Model: 87 [6/19 (32%)]\tLoss: 1.925512\n",
      "Train Model: 87 [11/19 (58%)]\tLoss: 1.566447\n",
      "Train Model: 87 [16/19 (84%)]\tLoss: 1.275499\n",
      "Train Model: 87 [19/19 (100%)]\tLoss: 0.676592\n",
      "Training model: 35\n",
      "Train Model: 35 [1/19 (5%)]\tLoss: 1.801376\n",
      "Train Model: 35 [6/19 (32%)]\tLoss: 2.155612\n",
      "Train Model: 35 [11/19 (58%)]\tLoss: 1.664561\n",
      "Train Model: 35 [16/19 (84%)]\tLoss: 1.513425\n",
      "Train Model: 35 [19/19 (100%)]\tLoss: 1.429729\n",
      "Training model: 21\n",
      "Train Model: 21 [1/19 (5%)]\tLoss: 1.860287\n",
      "Train Model: 21 [6/19 (32%)]\tLoss: 2.063735\n",
      "Train Model: 21 [11/19 (58%)]\tLoss: 1.713889\n",
      "Train Model: 21 [16/19 (84%)]\tLoss: 1.177157\n",
      "Train Model: 21 [19/19 (100%)]\tLoss: 1.347866\n",
      "Training model: 40\n",
      "Train Model: 40 [1/19 (5%)]\tLoss: 1.916672\n",
      "Train Model: 40 [6/19 (32%)]\tLoss: 2.048329\n",
      "Train Model: 40 [11/19 (58%)]\tLoss: 1.471401\n",
      "Train Model: 40 [16/19 (84%)]\tLoss: 2.316228\n",
      "Train Model: 40 [19/19 (100%)]\tLoss: 0.982464\n",
      "\n",
      "Test set: Average loss: 1.6218, Accuracy: 7629/10000 (76%)\n",
      "\n",
      "FL Round: 4\n",
      "Training model: 19\n",
      "Train Model: 19 [1/19 (5%)]\tLoss: 1.524890\n",
      "Train Model: 19 [6/19 (32%)]\tLoss: 2.146735\n",
      "Train Model: 19 [11/19 (58%)]\tLoss: 1.536508\n",
      "Train Model: 19 [16/19 (84%)]\tLoss: 0.867611\n",
      "Train Model: 19 [19/19 (100%)]\tLoss: 0.875358\n",
      "Training model: 11\n",
      "Train Model: 11 [1/19 (5%)]\tLoss: 1.566692\n",
      "Train Model: 11 [6/19 (32%)]\tLoss: 1.626427\n",
      "Train Model: 11 [11/19 (58%)]\tLoss: 1.333087\n",
      "Train Model: 11 [16/19 (84%)]\tLoss: 1.122166\n",
      "Train Model: 11 [19/19 (100%)]\tLoss: 1.053928\n",
      "Training model: 20\n",
      "Train Model: 20 [1/19 (5%)]\tLoss: 1.693103\n",
      "Train Model: 20 [6/19 (32%)]\tLoss: 1.500366\n",
      "Train Model: 20 [11/19 (58%)]\tLoss: 1.543749\n",
      "Train Model: 20 [16/19 (84%)]\tLoss: 1.928741\n",
      "Train Model: 20 [19/19 (100%)]\tLoss: 0.966379\n",
      "Training model: 75\n",
      "Train Model: 75 [1/19 (5%)]\tLoss: 1.686951\n",
      "Train Model: 75 [6/19 (32%)]\tLoss: 1.732061\n",
      "Train Model: 75 [11/19 (58%)]\tLoss: 1.205316\n",
      "Train Model: 75 [16/19 (84%)]\tLoss: 0.771381\n",
      "Train Model: 75 [19/19 (100%)]\tLoss: 0.721309\n",
      "Training model: 58\n",
      "Train Model: 58 [1/19 (5%)]\tLoss: 1.661931\n",
      "Train Model: 58 [6/19 (32%)]\tLoss: 2.146024\n",
      "Train Model: 58 [11/19 (58%)]\tLoss: 1.606257\n",
      "Train Model: 58 [16/19 (84%)]\tLoss: 1.184194\n",
      "Train Model: 58 [19/19 (100%)]\tLoss: 1.162331\n",
      "Training model: 64\n",
      "Train Model: 64 [1/19 (5%)]\tLoss: 1.680870\n",
      "Train Model: 64 [6/19 (32%)]\tLoss: 2.235312\n",
      "Train Model: 64 [11/19 (58%)]\tLoss: 2.299030\n",
      "Train Model: 64 [16/19 (84%)]\tLoss: 1.519715\n",
      "Train Model: 64 [19/19 (100%)]\tLoss: 1.262619\n",
      "Training model: 4\n",
      "Train Model: 4 [1/19 (5%)]\tLoss: 1.702949\n",
      "Train Model: 4 [6/19 (32%)]\tLoss: 2.072372\n",
      "Train Model: 4 [11/19 (58%)]\tLoss: 1.475855\n",
      "Train Model: 4 [16/19 (84%)]\tLoss: 1.341074\n",
      "Train Model: 4 [19/19 (100%)]\tLoss: 1.084534\n",
      "Training model: 52\n",
      "Train Model: 52 [1/19 (5%)]\tLoss: 1.652026\n",
      "Train Model: 52 [6/19 (32%)]\tLoss: 2.152817\n",
      "Train Model: 52 [11/19 (58%)]\tLoss: 1.594695\n",
      "Train Model: 52 [16/19 (84%)]\tLoss: 1.008208\n",
      "Train Model: 52 [19/19 (100%)]\tLoss: 0.812364\n",
      "Training model: 23\n",
      "Train Model: 23 [1/19 (5%)]\tLoss: 1.715317\n",
      "Train Model: 23 [6/19 (32%)]\tLoss: 2.184488\n",
      "Train Model: 23 [11/19 (58%)]\tLoss: 1.506633\n",
      "Train Model: 23 [16/19 (84%)]\tLoss: 0.835869\n",
      "Train Model: 23 [19/19 (100%)]\tLoss: 1.443534\n",
      "Training model: 93\n",
      "Train Model: 93 [1/19 (5%)]\tLoss: 1.651945\n",
      "Train Model: 93 [6/19 (32%)]\tLoss: 1.676444\n",
      "Train Model: 93 [11/19 (58%)]\tLoss: 1.420150\n",
      "Train Model: 93 [16/19 (84%)]\tLoss: 0.749844\n",
      "Train Model: 93 [19/19 (100%)]\tLoss: 1.371760\n",
      "\n",
      "Test set: Average loss: 1.3910, Accuracy: 7757/10000 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training code goes here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}